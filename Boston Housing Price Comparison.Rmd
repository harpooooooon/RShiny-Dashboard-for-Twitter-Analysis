---
title: "Boston Housing Price Predictions: \n General Linear Regression vs. Neural Network"
author: "Nipunjeet Gujral"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
  theme: flatly
  highlight: haddock
---



#### Packages and Randomization

|    Library   |              Purpose             |
|:------------:|:--------------------------------:|
| neuralnet    | Neural Network Related Functions |
| tidyverse    | Data Manipulation                |        
| MASS         | Boston Housing Predictions       |
| set.seed(500)| Randomization Factor             |



```{r libraries, include=FALSE}
# install.packages("c(MASS", "neuralnet", "tidyverse"))
library(MASS)
library(neuralnet)
library(tidyverse)
set.seed(500)
```



### Cleaning the data
```{r cleaning data, message=FALSE, warning=FALSE}
data <- Boston %>%
  map(~.x) %>%
  discard(~all(is.na(.x))) %>%
  map_df(~.x)

```



### Constructing  Training and Testing Datasets
```{r splitting into training and test data sets, message=FALSE, warning=FALSE}

# splitting : train = 75% and test = 25% of total data
index <- sample(1:nrow(data), round(0.75*nrow(data)))
train <- data[index, ]
test <- data[-index, ]

# rescaling data from current range to interval [0,1]
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
  
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))

# fromally setting the test and train, rescaled datasets
neural_train <- scaled[index, ]
neural_test <- scaled[-index, ]
```



### Constructing and Plotting a General Linear Regression
```{r Linear Model, message=FALSE, warning=FALSE}
lm.fit <- glm(medv~., data = train)
pr.lm <- predict(lm.fit, test)
MSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test)

plot(lm.fit)
summary(lm.fit)
```



### Testing the Neural Network
```{r Testing the Neural Network, message=FALSE, warning=FALSE}
# setting input layer 
inputs <- names(neural_train)

# constructing the regression formula 
formula <- as.formula(paste('medv ~', 
                            paste(inputs[!inputs %in% 'medv'], 
                                  collapse = ' + ')))

# constructing the structure of a Neural Network
nn <- neuralnet(formula = formula, 
                data = neural_train,
                hidden = c(7, 5, 3),
                linear.output = TRUE)

# visualizing the Neural Network
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
par(mar = numeric(4), family = 'serif')
plot.nnet(nn)
# plot(nn, rep = 'best')
```


### Training the Neural Network
```{r Training the Neural Network}
# predicting the neuron threshold value
  predict.nn <- neuralnet::compute(nn, neural_test[ , 1:13])
  
  predict.nn.1 <- predict.nn$net.result*(max(data$medv) - min(data$medv)) + min(data$medv)
  
  # definning the MSE for the neural network to minimize 
  test.r <- (neural_test$medv)*(max(data$medv) - min(data$medv)) + min(data$medv)
  
  MSE.nn <- sum((test.r - predict.nn.1)^2)/nrow(neural_test)
```



### Comparing the Neural Network to the General Linear Regression
```{r Comparitive Summary}
print(paste(MSE.lm, MSE.nn))
  print(paste('Reduction in MSE cause by using a Neural Network: ', 
              round((1 - MSE.nn/MSE.lm)*100, 3), '%'))
  
```

